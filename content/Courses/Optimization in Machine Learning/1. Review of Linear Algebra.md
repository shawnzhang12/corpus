---
title: "1. Review of Linear Algebra"
weight: -20

---

### Vectors and Matrices

{{< katex display >}}
a = \begin{bmatrix} 1\\ 2\\ 3\\ \end{bmatrix} \in \R^3,  A^{m \times n} = 
\begin{bmatrix}
   a_{11} & a_{12} & \cdots & a_{1n}\\
   a_{21} & a_{22} & \cdots & a_{2n}\\
   \vdots & \vdots & \ddots & \vdots \\
   a_{m1} & a_{m2} &  \cdots & a_{mn}
\end{bmatrix}
{{< /katex >}}

### Matrix  Operations and Properties

#### Addition

The sum of two matrices {{< katex >}}A \in \R^{m \times n}, B \in \R^{m \times n}{{< /katex >}} is defined as the element-wise sum, i.e.,

{{< katex >}}A+B:= 
\begin{bmatrix}
   a_{11} + b_{11} & \cdots & a_{1n} + b_{1n}\\
   \vdots &  & \vdots\\
   a_{m1} + b_{m1} &  \cdots & a_{mn} + b_{mn}
\end{bmatrix} \in \R^{m \times n}{{< /katex >}}

#### Multiplication

Only if neighbouring dimensions match.

{{< katex display >}}\underbrace{A}_{n \times k} \underbrace{B}_{k \times m} = \underbrace{C}_{n \times m}{{< /katex >}}

#### Identity

{{< katex >}}I_{n} := 
\begin{bmatrix}
   1 & 0 & \cdots & 0\\
   0 & 1 & \cdots & 0\\
   \vdots & \vdots & \ddots & \vdots \\
   0 & 0 &  \cdots & 1
\end{bmatrix} \in \R^{n \times n}{{< /katex >}}

#### Properties

- *Associativity*:

  ​	{{< katex >}}\forall A \in \R^{m \times n}, B \in \R^{n \times p}, C \in \R^{p \times q}: (AB)C = A(BC){{< /katex >}}

  ​	For scalar multiplication, can move scalar values around anywhere.

- *Distributivity*:

  ​	{{< katex >}}\forall A, B \in \R^{m \times n}, C, D \in \R^{n \times p}: (A + B)C = AC + BC{{< /katex >}}

  ​														   {{< katex >}}A(C + D) = AC + AD{{< /katex >}}

  ​	Same scalar distributivity as real numbers.

- Multiplication with the identity matrix:

  ​	{{< katex >}}\forall A \in \R^{m \times n}: I_m A = AI_n = A{{< /katex >}}

  Note that {{< katex >}}I_m \neq I_n{{< /katex >}} for {{< katex >}}m \neq n{{< /katex >}}

### Systems of Linear Equations

##### Solution Space

1. One solution: Equations are linearly independent, # unknowns = # equations
2. No solution: Some combination of equations contradicts another equation
3. Infinite solutions: Equations are linearly dependent

![](systems.png)

##### Solving Linear Systems

-   Row-echelon decomposition

##### Linear Independence

First defining a linear combination. Consider a vector space {{< katex >}}V{{< /katex >}} and a finite number of vectors {{< katex >}}x_1, ...,x_k \in V{{< /katex >}}. Then, every {{< katex >}}v \in V{{< /katex >}} of the form 

{{< katex display >}}v=\lambda_1 x_1 + \cdots + \lambda_k x_k = \sum_{i = 1}^k \lambda_i x_i \in V{{< /katex >}}

with {{< katex >}}\lambda_1, ..., \lambda_k \in \R{{< /katex >}} is a <u>*linear combination*</u> of the vectors {{< katex >}}x_1, ..., x_k{{< /katex >}}.

Consider the same vector space {{< katex >}}V{{< /katex >}}. If there is a non-trivial linear combination, such that {{< katex >}}0 = \sum_{i=1}^k \lambda_i x_i{{< /katex >}} with at least one {{< katex >}}\lambda_i \neq 0{{< /katex >}}, the vectors {{< katex >}}x_1, ...,x_k{{< /katex >}} are *<u>linearly dependent</u>*. Otherwise, if the only solution is the trivial solution {{< katex >}}\lambda_1 = ... = \lambda_k = 0{{< /katex >}} the vectors <u>*linearly independent*</u>.

##### Convex Combination

Non-negative {{< katex >}}\lambda_i{{< /katex >}} such that {{< katex >}}\sum_{i=1}^n \lambda_i = 1{{< /katex >}}.

