---
title: "1. Distributed Systems Examples"
weight: -20
---

## Massively Scalable Key-Value Stores
![](../background.jpg)
#### What are key-value stores?

- Container for key-value pairs (databases)
- Distributed, multi-component
- Non-relational
- **Simpler query semantics** for increased scalability, speed, availability, and flexibility

Why needed?

**Big data**. Huge amount of internet users, stored data, frequent updates, fast retrievals. Good for failure detection, failure recovery, replication, memory store, versioning.

- <u>Horizontal scalability:</u> user growth, more nodes
- <u>Performance:</u> single-record read and write operations
- <u>Flexibility:</u> adapt to changing data definitions
- <u>Reliability:</u> provides failure recovery (failure is the norm), thousands of components at play
- <u>Availability and geo-distribution</u>: Users are worldwide, data can be accessed worldwide

**Main operations**: **PUT**, **GET**, and **DELETE**. That's it.

Some key-value store in practice: BigTable, HBase, Cassandra, Redis, Dynamo

## BigTable 

Google, Based on Google File System. Designed for petabyte scale. Tables and tablets (smaller 200MB table partition)

- Client Library
- Master (controller, single point of failure)
  - Metadata operations
  - Load balancing
- Tablet server
  - Data operations

![](../bigtable.png)

#### Master

Assigns tablets to tablet servers. Load balancing. Detects addition and expiration of tablet servers.

#### Tablet Server

Manage set of tablets. Handles read and write requests for its tablets. Splits large tablets.

#### GFS

Distributed file system. BigTable is just using it for data, log storage, replication.

#### Chubby

Coordination, Lock service, metadata storage. A little filesystem itself.

#### Scheduler

Monitoring, failover, allocates resources (usually virtual now).

### Tablet location hierarchy

Heavy caching. Chubby file {{< katex >}}\rightarrow{{< /katex >}} Root tablet with tablet METADATA {{< katex >}}\rightarrow{{< /katex >}} More METADATA tablets {{< katex >}}\rightarrow{{< /katex >}} User tablets.

With 3 levels ({{< katex >}}1, 2^{17}, 2^{17}{{< /katex >}}), we have {{< katex >}}2^{34}{{< /katex >}} tablets

## Apache HBase

Open source re-implementation of BigTable.

- GFS {{< katex >}}\rightarrow{{< /katex >}} HDFS
- Chubby {{< katex >}}\rightarrow{{< /katex >}} ZooKeeper
- BigTable {{< katex >}}\rightarrow{{< /katex >}} HBase
- MapReduce {{< katex >}}\rightarrow{{< /katex >}} Hadoop

### Architecture Overview

- Client library
  - Issues put, get, delete operations
- ZooKeeper (Chubby)
  - Distributed lock service for HBase components
  - Based on ZAB - ZooKeeper Atomic Broadcast (Paxos)
- HRegion (tablet)
  - tables are split into multiple key-regions
- HRegion Server (tablet server)
  - processes operations for data / key-regions (tablets)
  - can host multiple key-regions (tablets)
- HMaster (master)
  - Coordinates components
    - startup, shutdown, failure of region servers 
    - opens, closes, assigns, moves regions
  - Not on read or write path
- Write Ahead Log (WAL)
  - Persists log of operations before anything happens for failure recovery
- MemStore
  - keep data in main memory, periodically sync to disk
- HDFS (GFS)
  - underlying distributed file system, table data stored as HFile format
  - Replicates data over multiple data nodes

### HBase read-path

![](../hbase_readpath.jpg)

1. Get ROOT tablet from zookeeper
2. Get metadata of tablets pertaining to table 1
3. Narrows down to key ranges (which specific tablet)
4. Get value

### HBase write-path

![](../hbase_writepath.png)

1. Same four steps for read path
2. Send put to Region Server
3. Write key-value pair to WAL 
4. Write key-value pair to MemStore
5. Occasionally flush key-value pair to HDFS (for fast read and write)

### HBase Scalability

- add multiple backup master servers
  - avoid single point of failure
  - leader election using zookeeper
- components can be added on-the-fly
- Add multiple region servers
  - horizontal scalability
  - master takes care of load balancing



![](../hbase_scalability.png)

When regions get too big, they split, new region registered in zookeeper, master is notified, and meta-table is updated.

### Hbase storage unit failure

Zookeeper will notice when TCP connection to a server expires and will delete node. Master will take the WAL (how?) and recover the server by replaying the WAL. WAL is stored in the distributed file system.

![](../hbase_failure.png)

## Summary on BigTable and Hbase

- Partitioning of data for **horizontal scalability**
  - tables {{< katex >}}\rightarrow{{< /katex >}} tablets/regions
  - load-balanced amongst region servers
  - Write-Ahead-Log for failure recovery
  - Decouple write from actual I/O of value to disk
  - Use MemStore for fast write
- Centralized management
  - One master, backup masters with leader election
  - Not involved in read/write path
- Coordination
  - Zookeeper (Chubby) lock service 
  - leader election, failure detection, paxos
  - cache meta-data replies to avoid frequent communication
  - high availability and reliability
- Distributed file system
  - HDFS stored as Hfiles
  - Data is replicated for availability

## Chubby Lock Service

Stores **small amount of information** for **high availability**. Metadata of system.

Bigtable can be recovered as long as Chubby is still available.

### Lock Service Operational Model

- Manages directories, files, read/writes are **atomic**.
- Clients maintain **sessions**, if sessions expires then **locks are released**.

### Lock Service Availability

- five active replicas
- one replica is designated as master, need to elect master
  - Chubby master and BigTable master are **different**!
- Service is up when majority of replicas are running (can communicate with each other)

#### Example: Leader election

See who can acquire an exclusive lock on a file first.

1. Clients concurrently open a file and attempt to acquire the file lock in write mode
2. One client succeeds, writes its name as the leader
3. Rest of the clients (become replicas) subscribe to modification event, read leader from file

## Cassandra :eye:

Based on Amazon Dynamo, developed by Facebook. Structured storage nodes (no GFS). Decentralized architecture (no master), consistent hashing for load balancing, gossiping to exchange information.

Each node is responsible for a key interval. Each node has a commit log, memtable and string tables. Can survive failure from commit log, which it occassionally moves data to disk and clears log.

#### Global Read Path

![](../cassandra_read.png)

1. Client sends request to any node
2. The new coordinator node determines responsible replica and sends request
3. Replica queries local file system and returns value to coordinator
4. Coordinator returns value to client

#### Global Write Path

![](../cassandra_write.png)

1. Client request, designates coordinator node
2. Coordinate determines replicas, and sends request to them
3. Replicas acknowledge write, and if majority of them updates back with coordinator than coordinator responds to client

#### Incremental Scaling 

![](../cassandra_scaling.png)

1. Node picks random location
2. Key range for next node is split in half
3. New node location information is gossiped

#### Failure

![](../cassandra_failure.png)

1. Node crashes
2. Neighbor nodes determine node crashed: failure gossiped around
3. Recovery of redundant information from other nodes 









