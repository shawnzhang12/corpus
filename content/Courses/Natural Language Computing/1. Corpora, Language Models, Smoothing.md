---
title: "1. Corpora, Language Models, Smoothing"
weight: -20
---

What are we counting? Tokens and types. Instances of words and kinds of words.

**Corpora:** A body of language data. Most useful when occurring naturally. Be aware of bias. 

**Vocabulary**: Set of unique Words

**Size of corpus**: Total number of words

### Word Order Matters!

Should consider past words, but not ALL past words. {{< katex >}}P(car|the\:old) = 0{{< /katex >}} may never appear in the corpus, but {{< katex >}}P(old|the) > 0{{< /katex >}} and {{< katex >}}P(car|old) > 0{{< /katex >}} could be true, {{< katex >}}\therefore P(the\:old\:car) > 0{{< /katex >}}.

#### Solution: Word prediction with N-gram Models

**Language Models:** The statistical model of a language. Probability of words in an *ordered* sequence. i.e., {{< katex >}}P(w_1, w_2, ..., w_n){{< /katex >}}.

**N-grams:** token sequences of length N

Given probabilities of N-grams, we can compute the conditional probabilities of possible subsequent words. Estimate {{< katex >}}P(w_t|w_{t-1}){{< /katex >}} from count of {{< katex >}}(w_{t-1}, w_t){{< /katex >}} in corpus. 

Example: 'the last word in this sentence is ___'. What is the next word? Suppose we know, 

{{< katex display >}}P(is\:the) > P(is\:a) \therefore P(the|is) > P(a|is){{< /katex >}}

Then we would predict 'the last word in this sentence is **the**'.

**Term count (Count):** the number of tokens of term {{< katex >}}w{{< /katex >}} in {{< katex >}}C{{< /katex >}}

**Relative frequency ({{< katex >}}F_C{{< /katex >}}):** relative to total number of tokens, basically the fraction of {{< katex >}}\frac{count}{total\:count}{{< /katex >}}.

**Probabilistic Chain Rule:** {{< katex display >}}P(A,B,C,D)=P(A)P(B|A)P(C|A,B)P(D|A,B,C){{< /katex >}}

But there are *many* possible solutions, how do we avoid 0 probabilities? We can make a Markov assumption, that only a short linear history of length L is sufficient for word prediction.

**Bigram** version: {{< katex display >}}P(w_n|w_{1:(n-1)}) \approx P(w_n|w_{n-1}){{< /katex >}}

### Bigram  Counts and Probabilities

![](bigram_counts.png)

To get likelihoods, we can dividing bigram counts by unigram counts. 

![](bigram_probabilities.png)

Now we can estimate the probability of **whole sentences** by adding the start (\<s\>) and end (\</s\>) tags.

{{< katex display >}}P(<s> I\:want\:english\:food</s>) \approx{{< /katex >}}

{{< katex display >}}P(I|<s>)P(want|I)P(english|want)P(food|english)P(</s>|food) \approx 0.000031{{< /katex >}}

#### Are N-grams still relevant?

Often cheaper to train/query than neural LMs. Can be interpolated with neural LMs to often achieve SOTA performance. *Occasionally* outperform neural LMs. Interpretable and convenient.

## Evaluation of Language Models

### Shannon's method

Sample based on context. RNG those darts. In general, Quadrigrams > Trigrams > Bigrams > Unigrams for appearing more similar to actual texts like Shakespeare. This is because for increasing context, there are fewer possible next words. E.g. {{< katex >}}P(Gloucester|seek\:the\:traitor) = 1{{< /katex >}}.

![](shannon.png)

### Goodness of a Model?

#### Extrinsic Evaluation

Utility of a language model is often determined *in situ* (in practice). Take two different LMs and compare them, such as for speech recognition.

#### Intrinsic Evaluation

Estimate the probability of a corpus {{< katex >}}P(C){{< /katex >}}.

### Maximum Likelihood Estimate

{{< katex >}}\theta^* = \text{argmax}_{\theta}L_M(\theta | T), L_M(\theta|T) = P_{M(\theta)}(T){{< /katex >}}, where T is the Brown corpus, M is the bigram and unigram tables, and parameters {{< katex >}}\theta_{to|want} \:\text{is}\: P(to|want){{< /katex >}}.

### Perplexity

Sort of like a 'branching factor', {{< katex >}}PP(C) = 2^{-(\frac{log_2P(C)}{||C||})} = P(C)^{-1/||C||}{{< /katex >}}

Minimizing perplexity {{< katex >}}\equiv{{< /katex >}} Maximizing probability of corpus.

**Max perplexity**: Uniform corpus distribution

**Lower** perplexity {{< katex >}}\rightarrow{{< /katex >}} **Better** Model. Trigram has lowest perplexity compared to bigram.

## Zipf and the Natural Distributions in Language

### Sparseness

Longer n-grams will result in more sparsity. Will result in probabilities of 0 when you do multiplications.

### Patterns of Unigrams

A few words occur **very frequently**.

#### Frequency of frequencies

![](freqoffreq.png)

#### Word Rankings

![](wordrankings.png)

### Zipf's Law

<u>We humans are Nlazy.</u>

Speaker minimizes effort by having a **small** vocabulary of **common** words.

Hearer minimizes effort by having a **large** vocabulary of **less ambiguous** words.

Compromise: frequency and rank are inversely proportional.

{{< katex display >}}f \propto \frac{I}{r} \quad \text{i.e., for some k} \quad f \cdot r = k{{< /katex >}}

Zero prob in Shakespeare, some N-grams are just really rare

### Smoothing mechanisms

#### 1. Add-{{< katex >}}\delta{{< /katex >}} smoothing (Laplace)

##### Laplace Smoothing ({{< katex >}}\delta{{< /katex >}} = 1)

+1 imaginary count to all words, no more zero counts!

Laplace smoothing results in non-zero probabilities which is great, but now the rich n-grams have reduced probabilities. Inflation and taxes! Robinhood. Steal from the rich and give to the poor.

E.g. {{< katex >}}P(to|want){{< /katex >}} went from 0.66 to 0.26 :disappointed:. That's a huge change!

In **extrinsic** evaluations, the results are not great.

##### Generalized Laplace ({{< katex >}}\delta{{< /katex >}} < 1)

Sometimes work empirically (e.g. text categorization), sometimes not.

#### 2. Good-Turing

**Idea**: We still want to get rid of zeros. But now we want re-estimate counts of words using the MLE of words that occur count + 1 times. We look to our word neighbors in terms of frequency to make new estimates. 

What is the probability of seeing something **new** ({{< katex >}}N_0{{< /katex >}})? Use the {{< katex >}}N_1{{< /katex >}} counts as an estimate. If {{< katex >}}N_{c+1}{{< /katex >}} has a count of zero, we can use log-log linearly interpolation to guess. 

![](turing_example.png)

![](turing_adjustments.png)

#### 3. Simple Interpolation

Combine trigram, bigram, and unigram probabilities.

![](interp.png)

#### 4. Absolute discounting

Instead of multiplying highest N-gram by a {{< katex >}}\lambda_i{{< /katex >}}, we can just subtract a fixed discount {{< katex >}}0 \leq \delta \leq 1{{< /katex >}} from each non-zero count. Both simple interpolation and absolute discounting redistribute probability mass, but for absolute discounting we have a similar trend observed when comparing counts of training set and held-out set.

#### 5. Kneser-Ney smoothing (Modified)

Use different absolute discounts depending on n-gram count. Lots of math that won't be tested, but Modified Kneser-Ney is arguably the **most popular choice** for n-gram language modelling.





