---
title: "2. Corpora, Language Models, Smoothing"
weight: -20
---

### Maximum Likelihood Estimate

eueu

### Perplexity

Equation here.

Minimizing perplexity $\equiv$ maximizing probability of corpus

Uniform corpus distribution = max perplexity

**Lower** perplexity $\rightarrow$ **Better** Model

trigram has lowest perplexity compared to bigram

## Zipf and the Natural Distributions in Language

### Sparseness

Longer n-grams will result in more sparsity. Will result in probabilities of 0 when you do multiplications.

### Patterns of Unigrams

A few words occur **very frequently**.

Frequency of frequencies:

cool chart, ranking words

### Zipf's Law

<u>We humans are lazy.</u>

Speaker minimizes effort by having a **small** vocabulary of **common** words.

Hearer minimizes effort by having a **large** vocabulary of **less ambiguous** words.

Compromise: frequency and rank are inversely proportional.

$$f \propto \frac{I}{r} \quad \text{i.e., for some k} \quad f \cdot r = k$$

Zero prob in Shakespeare, some N-grams are just really rare

### Smoothing mechanisms

#### 1. Add-$\delta$ smoothing (Laplace)

##### Laplace Smoothing ($\delta$ = 1)

+1 imaginary count to all words, no more zero counts!

Laplace smoothing results in non-zero probabilities which is great, but now the rich n-grams have reduced probabilities. Inflation and taxes! Robinhood. Steal from the rich and give to the poor.

E.g. $P(to|want)$ went from 0.66 to 0.26 :disappointed:. That's a huge change!

In **extrinsic** evaluations, the results are not great.

##### Generalized Laplace ($\delta$ < 1)

Sometimes work empirically (e.g. text categorization), sometimes not.

#### 2. Good-Turing

**Idea**: We still want to get rid of zeros. But now we want re-estimate counts of words using the MLE of words that occur count + 1 times. We look to our word neighbors in terms of frequency to make new estimates.

What is the probability of seeing something **new** ($N_0$)?

use n1

#### 3. Simple Interpolation

Combine trigram, bigram, and unigram probabilities.

Sum $\lambda$ = 1 constitutes a real distribution.

#### 4. Absolute discounting

Instead of multiplying by a lambda, just subtract a fixed delta from each non-zero count.







#### 









